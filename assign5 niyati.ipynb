{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ARvhXl80yKzm",
        "outputId": "a08add4f-8207-489c-ab1b-ca77b740128b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Best parameters:\n",
            "Learning Rate: 0.1  | Alpha: 1e-05\n",
            "Maximum R2 Score: 0.9843988688748493\n"
          ]
        }
      ],
      "source": [
        "#Q1\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import make_regression\n",
        "\n",
        "# Generate synthetic dataset\n",
        "X, y = make_regression(n_samples=200, n_features=7, noise=10, random_state=42)\n",
        "X = (X - X.mean(axis=0)) / X.std(axis=0)  # feature scaling\n",
        "y = y.reshape(-1, 1)\n",
        "\n",
        "# Add bias column\n",
        "X = np.c_[np.ones(X.shape[0]), X]\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "def ridge_regression_gd(X, y, lr=0.01, alpha=0.1, epochs=1000):\n",
        "    m, n = X.shape\n",
        "    W = np.zeros((n, 1))\n",
        "    for epoch in range(epochs):\n",
        "        y_pred = X @ W\n",
        "        gradient = (1/m) * (X.T @ (y_pred - y) + alpha * W)\n",
        "        W -= lr * gradient\n",
        "    return W\n",
        "\n",
        "learning_rates = [0.0001, 0.001, 0.01, 0.1, 1.0]\n",
        "alphas = [1e-15, 1e-10, 1e-5, 10, 100]\n",
        "\n",
        "best_r2 = -np.inf\n",
        "best_params = None\n",
        "\n",
        "for lr in learning_rates:\n",
        "    for alpha in alphas:\n",
        "        W = ridge_regression_gd(X_train, y_train, lr, alpha, epochs=1000)\n",
        "        y_pred = X_test @ W\n",
        "        r2 = r2_score(y_test, y_pred)\n",
        "        if r2 > best_r2:\n",
        "            best_r2 = r2\n",
        "            best_params = (lr, alpha)\n",
        "\n",
        "print(\"âœ… Best parameters:\")\n",
        "print(\"Learning Rate:\", best_params[0], \" | Alpha:\", best_params[1])\n",
        "print(\"Maximum R2 Score:\", best_r2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w7eygxkayQ3k",
        "outputId": "2f2c170c-7324-4ec4-a081-41e390df0986"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Dataset loaded successfully!\n",
            "          Unnamed: 0  AtBat  Hits  HmRun  Runs  RBI  Walks  Years  CAtBat  \\\n",
            "0     -Andy Allanson    293    66      1    30   29     14      1     293   \n",
            "1        -Alan Ashby    315    81      7    24   38     39     14    3449   \n",
            "2       -Alvin Davis    479   130     18    66   72     76      3    1624   \n",
            "3      -Andre Dawson    496   141     20    65   78     37     11    5628   \n",
            "4  -Andres Galarraga    321    87     10    39   42     30      2     396   \n",
            "\n",
            "   CHits  ...  CRuns  CRBI  CWalks  League Division PutOuts  Assists  Errors  \\\n",
            "0     66  ...     30    29      14       A        E     446       33      20   \n",
            "1    835  ...    321   414     375       N        W     632       43      10   \n",
            "2    457  ...    224   266     263       A        W     880       82      14   \n",
            "3   1575  ...    828   838     354       N        E     200       11       3   \n",
            "4    101  ...     48    46      33       N        E     805       40       4   \n",
            "\n",
            "   Salary  NewLeague  \n",
            "0     NaN          A  \n",
            "1   475.0          N  \n",
            "2   480.0          A  \n",
            "3   500.0          N  \n",
            "4    91.5          N  \n",
            "\n",
            "[5 rows x 21 columns] \n",
            "\n",
            "Linear Regression:\n",
            "  MSE = 150540.93,  RÂ² = 0.1677\n",
            "\n",
            "Ridge Regression:\n",
            "  MSE = 150168.56,  RÂ² = 0.1698\n",
            "\n",
            "Lasso Regression:\n",
            "  MSE = 154316.19,  RÂ² = 0.1468\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Q2\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/JWarmenhoven/ISLR-python/master/Notebooks/Data/Hitters.csv\"\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "print(\"âœ… Dataset loaded successfully!\")\n",
        "print(df.head(), \"\\n\")\n",
        "\n",
        "# 1. Drop rows with missing values\n",
        "df = df.dropna()\n",
        "\n",
        "# 2. Convert categorical columns to numeric (One-Hot Encoding)\n",
        "df = pd.get_dummies(df, drop_first=True)\n",
        "\n",
        "X = df.drop(\"Salary\", axis=1)\n",
        "y = df[\"Salary\"]\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "\n",
        "# Linear Regression\n",
        "lin_reg = LinearRegression()\n",
        "lin_reg.fit(X_train, y_train)\n",
        "y_pred_lin = lin_reg.predict(X_test)\n",
        "\n",
        "# Ridge Regression\n",
        "ridge_reg = Ridge(alpha=0.5748)\n",
        "ridge_reg.fit(X_train, y_train)\n",
        "y_pred_ridge = ridge_reg.predict(X_test)\n",
        "\n",
        "# Lasso Regression\n",
        "lasso_reg = Lasso(alpha=0.5748)\n",
        "lasso_reg.fit(X_train, y_train)\n",
        "y_pred_lasso = lasso_reg.predict(X_test)\n",
        "\n",
        "\n",
        "def evaluate_model(name, y_true, y_pred):\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "    print(f\"{name}:\\n  MSE = {mse:.2f},  RÂ² = {r2:.4f}\\n\")\n",
        "\n",
        "evaluate_model(\"Linear Regression\", y_test, y_pred_lin)\n",
        "evaluate_model(\"Ridge Regression\", y_test, y_pred_ridge)\n",
        "evaluate_model(\"Lasso Regression\", y_test, y_pred_lasso)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2nBpIundynA_",
        "outputId": "917359ee-3026-4476-c25d-17bfbda13f2c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-1696674157.py:16: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n",
            "  housing_tgz.extractall(path=\"housing\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Dataset loaded successfully!\n",
            "   longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
            "0    -122.23     37.88                41.0        880.0           129.0   \n",
            "1    -122.22     37.86                21.0       7099.0          1106.0   \n",
            "2    -122.24     37.85                52.0       1467.0           190.0   \n",
            "3    -122.25     37.85                52.0       1274.0           235.0   \n",
            "4    -122.25     37.85                52.0       1627.0           280.0   \n",
            "\n",
            "   population  households  median_income  median_house_value ocean_proximity  \n",
            "0       322.0       126.0         8.3252            452600.0        NEAR BAY  \n",
            "1      2401.0      1138.0         8.3014            358500.0        NEAR BAY  \n",
            "2       496.0       177.0         7.2574            352100.0        NEAR BAY  \n",
            "3       558.0       219.0         5.6431            341300.0        NEAR BAY  \n",
            "4       565.0       259.0         3.8462            342200.0        NEAR BAY  \n",
            "\n",
            "ðŸ”¹ RidgeCV Results:\n",
            "Best alpha: 10.0\n",
            "RÂ² score: 0.6488297874504785\n",
            "RMSE: 69298.74561122917\n",
            "\n",
            "ðŸ”¹ LassoCV Results:\n",
            "Best alpha: 100.0\n",
            "RÂ² score: 0.6487456749069123\n",
            "RMSE: 69307.04435718994\n"
          ]
        }
      ],
      "source": [
        "# Q3. RidgeCV and LassoCV on Housing Dataset\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import urllib.request\n",
        "import tarfile\n",
        "from sklearn.linear_model import RidgeCV, LassoCV\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# --- Step 1: Download and extract dataset ---\n",
        "url = \"https://raw.githubusercontent.com/ageron/handson-ml/master/datasets/housing/housing.tgz\"\n",
        "urllib.request.urlretrieve(url, \"housing.tgz\")\n",
        "\n",
        "with tarfile.open(\"housing.tgz\") as housing_tgz:\n",
        "    housing_tgz.extractall(path=\"housing\")\n",
        "\n",
        "# Load the CSV file\n",
        "df = pd.read_csv(\"housing/housing.csv\")\n",
        "print(\"âœ… Dataset loaded successfully!\")\n",
        "print(df.head())\n",
        "\n",
        "# --- Step 2: Data Preprocessing ---\n",
        "# Handle missing values\n",
        "df = df.dropna()\n",
        "\n",
        "# Convert categorical column 'ocean_proximity' to numeric using one-hot encoding\n",
        "df = pd.get_dummies(df, drop_first=True)\n",
        "\n",
        "# Separate features and target\n",
        "X = df.drop(\"median_house_value\", axis=1)\n",
        "y = df[\"median_house_value\"]\n",
        "\n",
        "# Scale the features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# --- Step 3: Split into training and testing data ---\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# --- Step 4: RidgeCV Implementation ---\n",
        "ridge = RidgeCV(alphas=np.logspace(-3, 3, 7), cv=5)\n",
        "ridge.fit(X_train, y_train)\n",
        "y_pred_ridge = ridge.predict(X_test)\n",
        "\n",
        "print(\"\\nðŸ”¹ RidgeCV Results:\")\n",
        "print(\"Best alpha:\", ridge.alpha_)\n",
        "print(\"RÂ² score:\", r2_score(y_test, y_pred_ridge))\n",
        "print(\"RMSE:\", np.sqrt(mean_squared_error(y_test, y_pred_ridge)))\n",
        "\n",
        "# --- Step 5: LassoCV Implementation ---\n",
        "lasso = LassoCV(alphas=np.logspace(-3, 3, 7), cv=5, max_iter=10000)\n",
        "lasso.fit(X_train, y_train)\n",
        "y_pred_lasso = lasso.predict(X_test)\n",
        "\n",
        "print(\"\\nðŸ”¹ LassoCV Results:\")\n",
        "print(\"Best alpha:\", lasso.alpha_)\n",
        "print(\"RÂ² score:\", r2_score(y_test, y_pred_lasso))\n",
        "print(\"RMSE:\", np.sqrt(mean_squared_error(y_test, y_pred_lasso)))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fR_mx-5IywcT",
        "outputId": "320a74da-f457-48d4-e87d-43b2bdc5ab81"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.9666666666666667\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        10\n",
            "           1       1.00      0.89      0.94         9\n",
            "           2       0.92      1.00      0.96        11\n",
            "\n",
            "    accuracy                           0.97        30\n",
            "   macro avg       0.97      0.96      0.97        30\n",
            "weighted avg       0.97      0.97      0.97        30\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "Q4\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Load data\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# One-vs-Rest Logistic Regression\n",
        "model = LogisticRegression(multi_class='ovr', solver='lbfgs', max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
