{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5687cc09",
   "metadata": {},
   "source": [
    "# Assignment 6 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39ec7b8",
   "metadata": {},
   "source": [
    "### Q1\n",
    "(Gaussian Naïve Bayes Classifier) Implement Gaussian Naïve Bayes Classifier on the Iris dataset from sklearn.datasets using \n",
    "(i) Step-by-step implementation \n",
    "(ii) In-built function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55a41927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussian NB (From Scratch)\n",
      "Accuracy: 0.9210526315789473\n",
      "Confusion Matrix:\n",
      " [[12  0  0]\n",
      " [ 0 12  1]\n",
      " [ 0  2 11]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "      setosa       1.00      1.00      1.00        12\n",
      "  versicolor       0.86      0.92      0.89        13\n",
      "   virginica       0.92      0.85      0.88        13\n",
      "\n",
      "    accuracy                           0.92        38\n",
      "   macro avg       0.92      0.92      0.92        38\n",
      "weighted avg       0.92      0.92      0.92        38\n",
      "\n",
      "Gaussian NB (sklearn)\n",
      "Accuracy: 0.9210526315789473\n",
      "Confusion Matrix:\n",
      " [[12  0  0]\n",
      " [ 0 12  1]\n",
      " [ 0  2 11]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "      setosa       1.00      1.00      1.00        12\n",
      "  versicolor       0.86      0.92      0.89        13\n",
      "   virginica       0.92      0.85      0.88        13\n",
      "\n",
      "    accuracy                           0.92        38\n",
      "   macro avg       0.92      0.92      0.92        38\n",
      "weighted avg       0.92      0.92      0.92        38\n",
      "\n",
      "Comparison\n",
      "From-scratch accuracy: 0.9211\n",
      "sklearn accuracy     : 0.9211\n",
      "\n",
      "Sample predicted probabilities (first 5 test samples):\n",
      "scratch:\n",
      " [[1.00000000e+000 3.05898525e-020 5.93927353e-027]\n",
      " [1.75843686e-135 6.39074555e-001 3.60925445e-001]\n",
      " [8.77614250e-088 9.99936121e-001 6.38785813e-005]\n",
      " [4.26275271e-083 9.99983856e-001 1.61444374e-005]\n",
      " [1.00000000e+000 1.24021403e-019 1.23201610e-026]]\n",
      "sklearn:\n",
      " [[1.00000000e+000 3.05898960e-020 5.93927941e-027]\n",
      " [1.75849960e-135 6.39074566e-001 3.60925434e-001]\n",
      " [8.77634283e-088 9.99936121e-001 6.38785928e-005]\n",
      " [4.26284535e-083 9.99983856e-001 1.61444405e-005]\n",
      " [1.00000000e+000 1.24021575e-019 1.23201730e-026]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data        \n",
    "y = iris.target      \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "class GaussianNBFromScratch:\n",
    "    def __init__(self, eps=1e-9):\n",
    "        self.eps = eps  # tiny value to avoid division by zero\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.classes_ = np.unique(y)\n",
    "        n_features = X.shape[1]\n",
    "        self.class_count_ = {}\n",
    "        self.class_prior_ = {}\n",
    "        self.theta_ = {}  # mean per class\n",
    "        self.sigma_ = {}  # variance per class\n",
    "        \n",
    "        for c in self.classes_:\n",
    "            X_c = X[y == c]\n",
    "            self.class_count_[c] = X_c.shape[0]\n",
    "            # prior = P(class)\n",
    "            self.class_prior_[c] = X_c.shape[0] / X.shape[0]\n",
    "            # mean (theta) and variance (sigma)\n",
    "            self.theta_[c] = np.mean(X_c, axis=0)\n",
    "            # use unbiased (ddof=0) population variance; add eps for stability\n",
    "            self.sigma_[c] = np.var(X_c, axis=0) + self.eps\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def _log_gaussian_prob(self, X, mean, var):\n",
    "        \"\"\"\n",
    "        Compute log of Gaussian probability density for each sample and feature.\n",
    "        For multivariate independent features, sum log-probabilities across features.\n",
    "        \"\"\"\n",
    "        # term1: -0.5 * log(2*pi*var)\n",
    "        term1 = -0.5 * np.log(2.0 * np.pi * var)\n",
    "        # term2: - (x - mean)^2 / (2*var)\n",
    "        term2 = - ((X - mean) ** 2) / (2.0 * var)\n",
    "        # sum over features to get log p(x | class) per sample\n",
    "        return np.sum(term1 + term2, axis=1)  # shape (n_samples,)\n",
    "    \n",
    "    def predict_log_proba(self, X):\n",
    "        \"\"\"\n",
    "        Returns log P(class) + log P(x | class) for each class and sample.\n",
    "        shape -> (n_samples, n_classes)\n",
    "        \"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        log_probs = np.zeros((n_samples, len(self.classes_)))\n",
    "        \n",
    "        for idx, c in enumerate(self.classes_):\n",
    "            log_prior = np.log(self.class_prior_[c])\n",
    "            log_likelihood = self._log_gaussian_prob(X, self.theta_[c], self.sigma_[c])\n",
    "            log_probs[:, idx] = log_prior + log_likelihood\n",
    "        \n",
    "        return log_probs\n",
    "    \n",
    "    def predict(self, X):\n",
    "        log_probs = self.predict_log_proba(X)\n",
    "        # pick class with highest posterior (in log-space)\n",
    "        indices = np.argmax(log_probs, axis=1)\n",
    "        return self.classes_[indices]\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Convert log_probs to normalized probabilities (softmax per sample).\n",
    "        \"\"\"\n",
    "        log_probs = self.predict_log_proba(X)\n",
    "        # For numerical stability, subtract max per row\n",
    "        a = log_probs - log_probs.max(axis=1, keepdims=True)\n",
    "        exp_a = np.exp(a)\n",
    "        probs = exp_a / exp_a.sum(axis=1, keepdims=True)\n",
    "        return probs\n",
    "\n",
    "# Fit the from-scratch model\n",
    "gnb_scratch = GaussianNBFromScratch()\n",
    "gnb_scratch.fit(X_train, y_train)\n",
    "\n",
    "y_pred_scratch = gnb_scratch.predict(X_test)\n",
    "acc_scratch = accuracy_score(y_test, y_pred_scratch)\n",
    "\n",
    "print(\"Gaussian NB (From Scratch)\")\n",
    "print(\"Accuracy:\", acc_scratch)\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_scratch))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_scratch, target_names=iris.target_names))\n",
    "\n",
    "gnb_sklearn = GaussianNB()\n",
    "gnb_sklearn.fit(X_train, y_train)\n",
    "y_pred_sklearn = gnb_sklearn.predict(X_test)\n",
    "acc_sklearn = accuracy_score(y_test, y_pred_sklearn)\n",
    "\n",
    "print(\"Gaussian NB (sklearn)\")\n",
    "print(\"Accuracy:\", acc_sklearn)\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_sklearn))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_sklearn, target_names=iris.target_names))\n",
    "\n",
    "print(\"Comparison\")\n",
    "print(f\"From-scratch accuracy: {acc_scratch:.4f}\")\n",
    "print(f\"sklearn accuracy     : {acc_sklearn:.4f}\")\n",
    "\n",
    "print(\"\\nSample predicted probabilities (first 5 test samples):\")\n",
    "print(\"scratch:\\n\", gnb_scratch.predict_proba(X_test[:5]))\n",
    "print(\"sklearn:\\n\", gnb_sklearn.predict_proba(X_test[:5]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc54b453",
   "metadata": {},
   "source": [
    "### Q2\n",
    "Explore about GridSearchCV toot in scikit-learn. This is a tool that is often used for tuning hyperparameters of machine learning models. Use this tool to find the best value of K for K-NN Classifier using any dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b1b69f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best K: 6\n",
      "Best parameters: {'metric': 'euclidean', 'n_neighbors': 6, 'weights': 'uniform'}\n",
      "Best cross-validation score: 0.9833333333333334\n",
      "Test accuracy: 0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "param_grid = {\n",
    "    \"n_neighbors\": list(range(1, 31)),  # Test k values from 1 to 30\n",
    "    \"weights\": [\"uniform\", \"distance\"],\n",
    "    \"metric\": [\"euclidean\", \"manhattan\"]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    knn,\n",
    "    param_grid,\n",
    "    cv=5,            # 5-fold cross validation\n",
    "    scoring=\"accuracy\"\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best K:\", grid_search.best_params_[\"n_neighbors\"])\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best cross-validation score:\", grid_search.best_score_)\n",
    "\n",
    "best_knn = grid_search.best_estimator_\n",
    "\n",
    "y_pred = best_knn.predict(X_test)\n",
    "print(\"Test accuracy:\", accuracy_score(y_test, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
